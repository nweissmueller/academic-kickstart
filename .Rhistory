blogdown::build_site()
blogdown::install_hugo()
blogdown::build_site()
knitr::opts_chunk$set(echo = TRUE)
##
# install.packages("TraMineR")
library(TraMineR)
data("mvad")
# Load the library, retrieve the mvad data and create a state sequence object
# from the status variables (columns 17 to 86):
mvad_alphab = c("employment", "FE", "HE", "joblessness","school", "training")
mvad_seq = seqdef(mvad, 17:86, xtstep = 6, alphabet = mvad_alphab)
# Compute pairwise optimal matching (OM) distances between sequences with an
# insertion/deletion cost of 1 and a substitution cost matrix based on observed
# transition rates:
mvad_om = seqdist(mvad_seq, method = "OM", indel = 1, sm = "TRATE")
# Proceed to an agglomerative hierarchical clustering using the obtained distance
# matrix, select the four-clusters solution and express it as a factor:
library(cluster)
clusterward = agnes(mvad_om, diss = TRUE, method = "ward")
mvad_cl4 = cutree(clusterward, k = 4)
cl4_lab = factor(mvad_cl4, labels = paste("Cluster", 1:4))
# Visualize the cluster patterns by plotting their transversal state distributions:
seqdplot(mvad_seq, group = cl4_lab, border = NA)
## Alluvial
library(tidyverse)
library(readr)
df = readr::read_csv(file = "pathway_alluvial.csv")
## alluvial
# install.packages("ggalluvial")
library(ggalluvial)
is_alluvia_form(df, axes = 1:3, silent = TRUE)
## plot
p = ggplot(df,aes(y = freq, axis1 = lot, axis2 = tx, axis3 = pathway)) +
geom_alluvium(aes(fill = pathway),
width = 0, knot.pos = 0, reverse = FALSE) +
guides(fill = FALSE) +
geom_stratum(width = 1/8, reverse = FALSE) +
geom_text(stat = "stratum", infer.label = TRUE, reverse = FALSE) +
scale_x_continuous(breaks = 1:3, labels = c("LoT", "Treatment", "Pathway")) +
theme_bw() +
# coord_flip()+
ggtitle("Tratment pathways by NCCN guidelines")
p
install.packages("traj")
library(traj)
knitr::opts_chunk$set(echo = TRUE)
## Load Data
path.data = paste(getwd(),sep="")
df.train = read.csv(paste(path.data,"/train.csv", sep = ""), header = T, sep = ",")
df.test = read.csv(paste(path.data,"/test.csv", sep = ""), header = T, sep = ",")
## Explore data distributions
colnames(df.train) = c("id", "price","year","bedrooms", "transport.prox",
"sqft", "year.built", "scl.qlty", "crime", "lot")
colnames(df.test) = c("id","year","bedrooms", "transport.prox",
"sqft", "year.built", "scl.qlty", "crime", "lot")
summary(df.train)
pairs(df.train)
## Check distribution of dependent variable
par(mfrow=c(2,5))
for(i in 1:10) {
hist(df.train[,i], main=names(df.train)[i])
}
## Check normality and the impact of variable transformations
## Likely, best result could be obtained using linear regression with optimal transformations
## could do this: https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html
library(ggplot2)
# Transformation 1
t1.price = log(df.train$price)
# Transformation 2
t2.price = 1/sqrt(df.train$price)
# Plot various transformations, if the result is a straight line, the distirbution is perfectly normal.
a = ggplot() +
geom_qq(aes(sample = df.train$price))
b = ggplot() +
geom_qq(aes(sample = t1.price))
c = ggplot() +
geom_qq(aes(sample = b.price$x.t))
par(mfrow = c(1,3))
plot(a)
plot(b)
plot(c)
## Check normality and the impact of variable transformations
## Likely, best result could be obtained using linear regression with optimal transformations
## could do this: https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html
library(ggplot2)
# Transformation 1
t1.price = log(df.train$price)
# Transformation 2
t2.price = 1/sqrt(df.train$price)
# Plot various transformations, if the result is a straight line, the distirbution is perfectly normal.
a = ggplot() +
geom_qq(aes(sample = df.train$price))
b = ggplot() +
geom_qq(aes(sample = t1.price))
c = ggplot() +
geom_qq(aes(sample = t2.price$x.t))
par(mfrow = c(1,3))
plot(a)
plot(b)
plot(c)
## Check normality and the impact of variable transformations
## Likely, best result could be obtained using linear regression with optimal transformations
## could do this: https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html
library(ggplot2)
# Transformation 1
t1.price = log(df.train$price)
# Transformation 2
t2.price = 1/sqrt(df.train$price)
# Plot various transformations, if the result is a straight line, the distirbution is perfectly normal.
a = ggplot() +
geom_qq(aes(sample = df.train$price))
b = ggplot() +
geom_qq(aes(sample = t1.price))
c = ggplot() +
geom_qq(aes(sample = t1.price$x.t))
par(mfrow = c(1,3))
plot(a)
plot(b)
plot(c)
## Check normality and the impact of variable transformations
## Likely, best result could be obtained using linear regression with optimal transformations
## could do this: https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html
library(ggplot2)
# Transformation 1
t1.price = log(df.train$price)
# Transformation 2
t2.price = 1/sqrt(df.train$price)
# Plot various transformations, if the result is a straight line, the distirbution is perfectly normal.
a = ggplot() +
geom_qq(aes(sample = df.train$price))
b = ggplot() +
geom_qq(aes(sample = t1.price))
c = ggplot() +
geom_qq(aes(sample = t2.price))
par(mfrow = c(1,3))
plot(a)
plot(b)
plot(c)
library(bestNormalize)
b.price = bestNormalize(df.train$price)
b.price
MASS::truehist(b.price$x.t, main = "Best transformation", nbins = 12)
b.price$chosen_transform$lambda
# Reverse box cox transformation
p = predict(b.price, inverse = TRUE)
## Variables ##
# Price: Box Cox
# Sqft: OderNorm
# Lot: No transformation
# Year: Order Norm
b.year.built = bestNormalize(df.train$year.built)
MASS::truehist(b.year.built$x.t, main = "Best transformation", nbins = 12)
# Year.built: Order Norm
# Bedrooms: no transformation
# Transport.prox: no transform
# Sclqlty: no transform
# Crime: no transform
# Test anova
fit = aov(price ~ year + bedrooms + transport.prox + sqft +
year.built + scl.qlty + crime + lot, data = df.train)
plot(fit)
summary(fit)
# Load packages
library(mlbench)
library(caret)
library(corrplot)
str(df.train)
# Visualize data
par(mfrow=c(2,5))
for(i in 1:10) {
hist(df.train[,i], main=names(df.train)[i])
}
# Transformation 1
# df.train$price = b.price$x.t
df.train$price = log(df.train$price)
# df.train$year.built = b.year.built$x.t
# df.train$sqft = log(df.train$sqft)
# df.train$crime = log(df.train$crime)
# df.train$lot = log(df.train$lot)
par(mfrow=c(2,5))
for(i in 1:10) {
hist(df.train[,i], main=names(df.train)[i])
}
library(corrplot)
cordata = cor(df.train[,3:10])
corrplot(cordata, method='circle')
library(corrplot)
cordata = cor(df.train[,3:10])
corrplot(cordata, method='square')
# Highly correlated variables
correlated = cor(df.train[,3:10])
highCorr = findCorrelation(correlated, cutoff=0.70)
# Identify variables that are highly correlated
names(df.train[highCorr])
# Remove ID variable from data frame
df.one = df.train[,-1] # take out ID
df.two = df.test[,-1]
# df.one.scale = scale(df.one)
# df.adj = df.one[,-8] # take out Crime
str(df.one)
## Change to categoriables
df.one$bedrooms = as.factor(df.one$bedrooms)
#df.one$year = as.factor(df.one$year)
#df.one$year.built = as.factor(df.one$year.built)
df.one$scl.qlty = as.factor(df.one$scl.qlty)
df.one$transport.prox = as.factor(df.one$transport.prox)
df.one$crime = as.factor(df.one$crime)
str(df.one)
# transform the test set too
df.two$bedrooms = as.factor(df.two$bedrooms)
#df.one$year = as.factor(df.one$year)
#df.one$year.built = as.factor(df.one$year.built)
df.two$scl.qlty = as.factor(df.two$scl.qlty)
df.two$transport.prox = as.factor(df.two$transport.prox)
df.two$crime = as.factor(df.two$crime)
str(df.two)
control = trainControl(method='repeatedcv', number=10, repeats=3)
metric = 'RMSE'
# Linear Regression (LR)
set.seed(44)
fit.lm = train(price~., data = df.one, method='lm', metric = metric,
preProc=c('center', 'scale'), trControl = control)
# Generalized Linear Regression (GLM)
set.seed(44)
fit.glm = train(price~., data = df.one, method='glm', metric = metric,
preProc=c('center', 'scale'), trControl = control)
# Penalized Linear Regression (GLMNET)
set.seed(44)
fit.glmnet = train(price~., data = df.one, method='glm', metric = metric,
preProc=c('center', 'scale'), trControl = control)
# Classification and Regression Trees (CART)
set.seed(44)
#grid = expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart = train(price~., data = df.one, method='rpart', metric = metric,
preProc=c('center', 'scale'), trControl = control)
# Support Vector Machines (SVM)
set.seed(44)
control = trainControl(method='repeatedcv', number=10, repeats=3)
metric = 'RMSE'
fit.svm = train(price~., data = df.one, method='svmRadial', metric = metric,
preProc=c('center', 'scale','BoxCox'), trControl = control)
# k-Nearest Neighbors (KNN)
set.seed(44)
fit.knn = train(price~., data = df.one, method='knn', metric = metric,
preProc=c('center', 'scale'), trControl = control)
# Compare the results of these algorithms
results = resamples(list(lm=fit.lm,
glm=fit.glm,
glmnet=fit.glmnet,
cart=fit.cart,
svm=fit.svm,
knn=fit.knn))
# Summary and Plot of Results
summary(results)
dotplot(results)
# Tune the SVM model to check performance: RADIAL
set.seed(44)
trainControl = trainControl(method='repeatedcv',number=10,repeats=3)
metric = 'RMSE'
svmGrid = expand.grid(.sigma=c(0.005, 0.01,0.015,0.0175,0.02,0.03),
.C = seq(5,40, by= 5))
svmModel = train(price~., data=df.train, method='svmRadial',
preProc=c('center', 'scale', 'BoxCox'), metric=metric, tuneGrid=svmGrid,
trControl=trainControl)
plot(svmModel)
svmModel$bestTune
# Tune the SVM model to check performance: LINEAR
set.seed(44)
trainControl = trainControl(method='repeatedcv',number=10,repeats=3)
metric = 'RMSE'
svmGrid = expand.grid(.C = seq(1,20, by= 1))
svmModelLin = train(price~., data=df.train, method='svmLinear',
preProc=c('center', 'scale','BoxCox'), metric=metric, tuneGrid=svmGrid,
trControl=trainControl)
plot(svmModelLin)
svmModelLin$bestTune
# Train the final model
# Support Vector Machines (SVM) - RADIAL
set.seed(44)
control = trainControl(method='repeatedcv', number=10, repeats=3)
metric = 'RMSE'
svmGrid = expand.grid(.sigma=c(0.01),
.C = 30)
fit.svm = train(price~., data=df.train, method='svmRadial',
preProc=c('center', 'scale','BoxCox'), metric=metric, tuneGrid=svmGrid,
trControl=trainControl)
# Precit the final price for the test set.
h_price = predict(fit.svm, newdata = df.test)
out = data.frame(Id = df.test$id, Predicted = round(exp(h_price),0))
row.names(out) = NULL
write.csv(out, file = "h_price_rad_v1.csv", row.names = FALSE)
# Generalized Linear Regression (GLM)
set.seed(44)
grid = expand.grid(alpha=c(0, 0.1, 0.2, 0.4), lambda = seq(0.01, 0.2, length=20))
fit.glmTune = train(price~., data=df.train, method='glmnet', metric=metric,
preProc=c('BoxCox'), tuneGrid=grid, trControl=trainControl)
# Generalized Linear Regression (GLM)
set.seed(44)
grid = expand.grid(alpha=c(0, 0.1, 0.2, 0.4), lambda = seq(0.01, 0.2, length=20))
fit.glmTune = train(price~., data=df.train, method='glmnet', metric=metric,
preProc=c('BoxCox'), tuneGrid=grid, trControl=trainControl)
plot(fit.glmTune)
fit.glmTune$bestTune
# Precit the final price for the test set.
h_price = predict(fit.glm, newdata = df.test)
trainControl = trainControl(method='repeatedcv', number=10, repeats=3)
metric = 'RMSE'
# Random Forest
set.seed(44)
fit.rf = train(price~., data=df.one, method='ranger', metric=metric,
trControl = trainControl)
# Gradient Boosting Machines (GBM)
set.seed(44)
fit.gbm = train(price~., data=df.one, method='gbm', metric=metric,
trControl = trainControl)
# CUBIST
set.seed(44)
fit.cubist = train(price~., data=df.one, method='cubist', metric=metric,
trControl = trainControl)
#
# set.seed(44)
# fit.lss = train(price~., data=df.one, method='svmSpectrumString', metric=metric,
#                trControl = trainControl)
# Compare Algorithms
ensembleResults = resamples(list(rf=fit.rf,
gbm=fit.gbm,
gbm.t=tune.gbm))
trainControl = trainControl(method='repeatedcv', number=10, repeats=3)
metric = 'RMSE'
# Random Forest
set.seed(44)
fit.rf = train(price~., data=df.one, method='ranger', metric=metric,
trControl = trainControl)
# Gradient Boosting Machines (GBM)
set.seed(44)
fit.gbm = train(price~., data=df.one, method='gbm', metric=metric,
trControl = trainControl)
# CUBIST
set.seed(44)
fit.cubist = train(price~., data=df.one, method='cubist', metric=metric,
trControl = trainControl)
#
# set.seed(44)
# fit.lss = train(price~., data=df.one, method='svmSpectrumString', metric=metric,
#                trControl = trainControl)
# Compare Algorithms
ensembleResults = resamples(list(rf=fit.rf,
gbm=fit.gbm,
gbm.t=fit.cubist))
#gbm.t2=tune.gbm2,
#xgb = fit.xgb,
#xgb.t=xgb_model))
#lss=fit.lss))
summary(ensembleResults)
dotplot(ensembleResults)
# TRy these:
# NO GO: 'bartMachine', 'ANFIS',
# GO:'bagEarth', 'brnn', 'bridge', 'blassoAveraged',  'monmlp','mxnet', 'neuralnet',
# 'qrnn', 'rbf', 'dnn', 'awtan'
# Generalized Linear Regression (GLM)
set.seed(44)
grid = expand.grid(alpha=c(0, 0.1, 0.2, 0.4), lambda = seq(0.01, 0.2, length=20))
fit.glmTune = train(price~., data=df.train, method='glmnet', metric=metric,
preProc=c('BoxCox'), tuneGrid=grid, trControl=trainControl)
plot(fit.glmTune)
fit.glmTune$bestTune
# Precit the final price for the test set.
h_price = predict(fit.glm, newdata = df.test)
# Generalized Linear Regression (GLM)
set.seed(44)
grid = expand.grid(alpha=c(0, 0.1, 0.2, 0.4), lambda = seq(0.01, 0.2, length=20))
fit.glmTune = train(price~., data=df.train, method='glmnet', metric=metric,
preProc=c('BoxCox'), tuneGrid=grid, trControl=trainControl)
plot(fit.glmTune)
fit.glmTune$bestTune
# Precit the final price for the test set.
h_price = predict(fit.glmTune, newdata = df.test)
out = data.frame(Id = df.test$id, Predicted = round(exp(h_price),0))
row.names(out) = NULL
# write.csv(out, file = "h_price_glmnet_v1.csv", row.names = FALSE)
# Test new methods
trainControl = trainControl(method='repeatedcv', number=10, repeats=3)
metric = 'RMSE'
# Random Forest
set.seed(44)
fit.f1 = train(price~., data=df.one, method='dnn', metric=metric,
trControl = trainControl)
# Gradient Boosting Machines (GBM)
set.seed(44)
fit.f2 = train(price~., data=df.one, method='bagEarth', metric=metric,
trControl = trainControl)
# CUBIST
set.seed(44)
fit.f3 = train(price~., data=df.one, method='brnn', metric=metric,
trControl = trainControl)
#
# set.seed(44)
# fit.lss = train(price~., data=df.one, method='svmSpectrumString', metric=metric,
#                trControl = trainControl)
# Compare Algorithms
ensembleResults = resamples(list(f1=fit.f1,
f2=fit.f2,
f3=fit.f3))
#lss=fit.lss))
summary(ensembleResults)
dotplot(ensembleResults)
# Trained Cubist model:
finalModel <- cubist(x=df.one, y=df.train$price, committees = 13)
shiny::runApp('~/OneDrive - Bristol-Myers Squibb (O365-D)/SOA_work/Documents/Move 3/Temp/KM mineR')
runApp('~/OneDrive - Bristol-Myers Squibb (O365-D)/Shared Files/KM-miner App')
runApp('~/OneDrive - Bristol-Myers Squibb (O365-D)/Shared Files/KM-miner App')
